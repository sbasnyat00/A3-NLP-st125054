This repo is for A3 of NLP 2025 AIT. 

For this assignment, I used [English-Nepali-Translation](https://www.kaggle.com/datasets/jigarpanjiyar/english-to-manipuri-dataset) Dataset from Kaggle to train Seq2Seq Transformer-based translation model for Nepali-English translation.

I compared three attention mechanisms: General, Multiplicative, and Additive Attention, analyzing their impact on training and validation loss, as well as perplexity. The results can be seen in this Report - 
[A3-ProjectReport-st125054.pdf](https://github.com/user-attachments/files/18637425/A3-ProjectReport-st125054.pdf)
